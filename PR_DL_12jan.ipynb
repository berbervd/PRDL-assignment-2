{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ea75b3-37ad-4e99-99d2-04178c491366",
   "metadata": {},
   "source": [
    "# Lezen van de files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6afa23-2777-430c-a3fb-536fb9a170c7",
   "metadata": {},
   "source": [
    "Importeren libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db3a75c-8195-40f9-a596-12a6a7f6fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151047a-87c1-4aa6-89f6-317a50691c3f",
   "metadata": {},
   "source": [
    "Hier 1 class van maken, doet:\n",
    "* lezen van de data\n",
    "* Specificeren van de folder waarvan je de data wil inlezen in folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dcde49-0ad3-4abb-a708-7c4e91c899a9",
   "metadata": {},
   "source": [
    "## Class voor all preprocess stappen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "412fbb08-7ad0-4718-83c9-d15e04080d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, base_directory='Final_Project_data/'):\n",
    "        self.base_directory = base_directory\n",
    "\n",
    "    def get_dataset_name(self, file_name_with_dir):\n",
    "        filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "        temp = filename_without_dir.split('_')[:-1]\n",
    "        dataset_name = \"_\".join(temp)\n",
    "        return dataset_name\n",
    "\n",
    "    def znorm(self, data):\n",
    "        \"\"\"\n",
    "        Normalizes time-wise\n",
    "        \"\"\"\n",
    "        mean_rows = np.mean(data, axis=1, keepdims=True)\n",
    "        std_rows = np.std(data, axis=1, keepdims=True)\n",
    "        scaled_data = ((data - mean_rows) / std_rows)\n",
    "        return scaled_data\n",
    "\n",
    "    def load_data_from_folder(self, folder, shuffle=True, downsample_factor=4):\n",
    "        data_directory = os.path.join(self.base_directory, folder)\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        label_mapping = {\n",
    "            'rest': 0,\n",
    "            'task_motor': 1,\n",
    "            'task_story_math': 2,\n",
    "            'task_working_memory': 3\n",
    "        }\n",
    "\n",
    "        file_names = [file_name for file_name in os.listdir(data_directory) if file_name.endswith(\".h5\")]\n",
    "        if shuffle:\n",
    "            random.shuffle(file_names)\n",
    "\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(data_directory, file_name)\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                dataset_name = self.get_dataset_name(file_name)\n",
    "                matrix = f.get(dataset_name)[()]\n",
    "\n",
    "                label = None\n",
    "                for task_prefix in label_mapping.keys():\n",
    "                    if task_prefix in file_name:\n",
    "                        label = label_mapping[task_prefix]  # Use the numerical value from label_mapping\n",
    "                        break\n",
    "\n",
    "                if label is not None:\n",
    "                    matrix = self.znorm(matrix)\n",
    "                    matrix = matrix[:, ::downsample_factor]\n",
    "\n",
    "                    data.append(matrix)\n",
    "                    labels.append(label)  # Append the label directly\n",
    "                else:\n",
    "                    print(f\"Warning: No label found for file {file_name}\")\n",
    "\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "# Example usage\n",
    "data_loader = DataLoader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ad1c1-69b1-43c1-9130-06d383e1ab01",
   "metadata": {},
   "source": [
    "### Inlezen Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "761a905b-61bf-46b8-9716-b189d6abbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (64, 248, 8906), Train Labels Shape: (64,)\n",
      "Test1 Data Shape: (16, 248, 8906), Test1 Labels Shape: (16,)\n",
      "Test2 Data Shape: (16, 248, 8906), Test2 Labels Shape: (16,)\n",
      "Test3 Data Shape: (16, 248, 8906), Test3 Labels Shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "# CROSS\n",
    "\n",
    "# Load data and labels for each subset\n",
    "data_train1, labels_train1 = data_loader.load_data_from_folder('Cross/train')\n",
    "data_test1, labels_test1 = data_loader.load_data_from_folder('Cross/test1')\n",
    "data_test2, labels_test2 = data_loader.load_data_from_folder('Cross/test2')\n",
    "data_test3, labels_test3 = data_loader.load_data_from_folder('Cross/test3')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Train Data Shape: {data_train1.shape}, Train Labels Shape: {labels_train1.shape}\")\n",
    "print(f\"Test1 Data Shape: {data_test1.shape}, Test1 Labels Shape: {labels_test1.shape}\")\n",
    "print(f\"Test2 Data Shape: {data_test2.shape}, Test2 Labels Shape: {labels_test2.shape}\")\n",
    "print(f\"Test3 Data Shape: {data_test3.shape}, Test3 Labels Shape: {labels_test3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4488d-3c04-4ac9-8ad5-1770f8f6cf1d",
   "metadata": {},
   "source": [
    "### Inlezen Intra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f838b5a9-39ae-4ad7-a68c-8c338ab2f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (32, 248, 8906)\n",
      "Labels Shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "# intra train\n",
    "\n",
    "# Load the preprocessed data and labels\n",
    "data_train, labels_train = data_loader.load_data_from_folder('Intra/train')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Data Shape: {data_train.shape}\")\n",
    "print(f\"Labels Shape: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56520b47-d362-4080-ad8d-3d1caf8d2181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (8, 248, 8906)\n",
      "Labels Shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "## Test intra\n",
    "# Load the preprocessed data and labels\n",
    "data_test, labels_test = data_loader.load_data_from_folder('Intra/test')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Data Shape: {data_test.shape}\")\n",
    "print(f\"Labels Shape: {labels_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d081997-15fd-4f3b-8ab6-e298629299aa",
   "metadata": {},
   "source": [
    "### Trainen + Maken van het model (intra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ded5ec6-c0d1-4fc6-a44c-d719004823bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshapen zodat het in de vorm: [nr samples, time steps, features] is, voor LSTM\n",
    "X_train = data_train1\n",
    "X_test = data_test1\n",
    "y_train = labels_train1\n",
    "y_test = labels_test1\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[2], X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[2], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1eadb6ed-0d22-4cf8-ba2e-9f388dc43810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/10-hyperparameters-to-keep-an-eye-on-for-your-lstm-model-and-other-tips-f0ff5b63fcd4\n",
    "# \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def create_model_oud(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "693d971b-b682-44d4-9080-253106dd3dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Obtaining dependency information for keras-tuner from https://files.pythonhosted.org/packages/2b/39/21f819fcda657c37519cf817ca1cd03a8a025262aad360876d2a971d38b3/keras_tuner-1.4.6-py3-none-any.whl.metadata\n",
      "  Downloading keras_tuner-1.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /opt/homebrew/lib/python3.11/site-packages (from keras-tuner) (2.14.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from keras-tuner) (23.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from keras-tuner) (2.31.0)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->keras-tuner) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->keras-tuner) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->keras-tuner) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->keras-tuner) (2023.7.22)\n",
      "Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8aab0d51-17b0-45ca-bf20-603668352f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 00s]\n",
      "\n",
      "Best val_accuracy So Far: None\n",
      "Total elapsed time: 00h 00m 01s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |3                 |num_lstm_layers\n",
      "8                 |52                |units_0\n",
      "1                 |3                 |num_dense_layers\n",
      "44                |52                |dense_units_0\n",
      "0.001             |0.001             |learning_rate\n",
      "32                |4                 |units_1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/dq/x1_4_7fd1lv6218h1vzxz8tr0000gn/T/ipykernel_10853/180573575.py\", line 11, in build_model\n",
      "    model.add(LSTM(\n",
      "              ^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/lstm.py\", line 562, in __init__\n",
      "    super().__init__(\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/dropout_rnn_cell_mixin.py\", line 43, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/base_rnn.py\", line 271, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 3820, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 452, in __init__\n",
      "    batch_input_shape = (batch_size,) + tuple(kwargs[\"input_shape\"])\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/dq/x1_4_7fd1lv6218h1vzxz8tr0000gn/T/ipykernel_10853/180573575.py\", line 11, in build_model\n    model.add(LSTM(\n              ^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/lstm.py\", line 562, in __init__\n    super().__init__(\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/dropout_rnn_cell_mixin.py\", line 43, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/base_rnn.py\", line 271, in __init__\n    super().__init__(**kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 3820, in __init__\n    super().__init__(**kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 452, in __init__\n    batch_input_shape = (batch_size,) + tuple(kwargs[\"input_shape\"])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not iterable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 47\u001b[0m\n\u001b[1;32m     41\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Add your data here\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# X_train, Y_train, X_val, Y_val = ...\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Start the hyperparameter tuning\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Get the optimal hyperparameters\u001b[39;00m\n\u001b[1;32m     50\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:586\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:543\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/dq/x1_4_7fd1lv6218h1vzxz8tr0000gn/T/ipykernel_10853/180573575.py\", line 11, in build_model\n    model.add(LSTM(\n              ^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/lstm.py\", line 562, in __init__\n    super().__init__(\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/dropout_rnn_cell_mixin.py\", line 43, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/layers/rnn/base_rnn.py\", line 271, in __init__\n    super().__init__(**kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 3820, in __init__\n    super().__init__(**kwargs)\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/tensorflow/python/trackable/base.py\", line 204, in _method_wrapper\n    result = method(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/jupyterlab/4.0.6/libexec/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 452, in __init__\n    batch_input_shape = (batch_size,) + tuple(kwargs[\"input_shape\"])\n                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tuning the number of LSTM layers and their units\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 3)):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('units_' + str(i), min_value=4, max_value=64, step=4),\n",
    "            return_sequences=i < hp.get('num_lstm_layers') - 1,  # Only the last layer should not return sequences\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]) if i == 0 else None))\n",
    "\n",
    "    # Tuning the number of Dense layers and their units\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int('dense_units_' + str(i), min_value=4, max_value=64, step=4),\n",
    "            activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='./',\n",
    "    project_name='hparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Add your data here\n",
    "# X_train, Y_train, X_val, Y_val = ...\n",
    "\n",
    "# Start the hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. \n",
    "The optimal number of LSTM and Dense layers and their units can be reviewed in the best hyperparameters.\n",
    "\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ae53766-6fad-421f-8429-ead3772f1d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1/1 [==============================] - 6s 6s/step - loss: 2.8125 - accuracy: 0.2969 - val_loss: 2.8125 - val_accuracy: 0.3750\n",
      "Epoch 2/7\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8125 - accuracy: 0.3438 - val_loss: 2.8125 - val_accuracy: 0.2500\n",
      "Epoch 3/7\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8125 - accuracy: 0.2812 - val_loss: 2.8125 - val_accuracy: 0.1875\n",
      "Epoch 4/7\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.8125 - accuracy: 0.2969 - val_loss: 2.8125 - val_accuracy: 0.1875\n",
      "Epoch 5/7\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8125 - accuracy: 0.2969 - val_loss: 2.8125 - val_accuracy: 0.1875\n",
      "Epoch 6/7\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8125 - accuracy: 0.3125 - val_loss: 2.8125 - val_accuracy: 0.1875\n",
      "Epoch 7/7\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.8125 - accuracy: 0.3125 - val_loss: 2.8125 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "model = create_model((X_train.shape[1], X_train.shape[2]))\n",
    "history = model.fit(X_train, y_train, epochs=7, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cccfe8b-28c5-471e-96f4-2a578484fd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step - loss: 2.8125 - accuracy: 0.2500\n",
      "Test Accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7c6a5b-baff-429a-883f-c0a2b275773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "79ae7df8-0251-4661-a85e-88de8b519123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 22 calls to <function Model.make_predict_function.<locals>.predict_function at 0x30af12f20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 222ms/step\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "1.6770587158425652\n"
     ]
    }
   ],
   "source": [
    "# predict:\n",
    "\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "\n",
    "#rmse \n",
    "#trainScore = np.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))\n",
    "#print(trainScore)\n",
    "testScore = np.sqrt(mean_squared_error(y_test, testPredict[:,0]))\n",
    "print(testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dd7e3-156e-4221-9c88-22108da01cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
