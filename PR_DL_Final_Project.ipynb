{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition and Deep Learning - Final Assignment\n",
    "\n",
    "Group Members:\n",
    "- Berber van Drunen (6396410)\n",
    "- Dean Newar (2755858)\n",
    "- Frederieke Blom (6433294)\n",
    "- Jens van der Weide (2492520)\n",
    "- Joeke Wolterbeek (6798942)\n",
    "- Jos Kisjes (2675293)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preprocessing class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, base_directory=''):\n",
    "        self.base_directory = base_directory\n",
    "\n",
    "    def get_dataset_name(self, file_name_with_dir):\n",
    "        filename_without_dir = file_name_with_dir.split('/')[-1]\n",
    "        temp = filename_without_dir.split('_')[:-1]\n",
    "        dataset_name = \"_\".join(temp)\n",
    "        return dataset_name\n",
    "\n",
    "    def znorm(self, data):\n",
    "        \"\"\"\n",
    "        Normalizes time-wise\n",
    "        \"\"\"\n",
    "        mean_rows = np.mean(data, axis=1, keepdims=True)\n",
    "        std_rows = np.std(data, axis=1, keepdims=True)\n",
    "        scaled_data = ((data - mean_rows) / std_rows)\n",
    "        return scaled_data\n",
    "\n",
    "    def load_data_from_folder(self, folder, shuffle=True, downsample_factor=4):\n",
    "        data_directory = os.path.join(self.base_directory, folder)\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        label_mapping = {\n",
    "            'rest': 0,\n",
    "            'task_motor': 1,\n",
    "            'task_story_math': 2,\n",
    "            'task_working_memory': 3\n",
    "        }\n",
    "\n",
    "        file_names = [file_name for file_name in os.listdir(data_directory) if file_name.endswith(\".h5\")]\n",
    "        if shuffle:\n",
    "            random.shuffle(file_names)\n",
    "\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(data_directory, file_name)\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                dataset_name = self.get_dataset_name(file_name)\n",
    "                matrix = f.get(dataset_name)[()]\n",
    "\n",
    "                label = None\n",
    "                for task_prefix in label_mapping.keys():\n",
    "                    if task_prefix in file_name:\n",
    "                        label = label_mapping[task_prefix]  # Use the numerical value from label_mapping\n",
    "                        break\n",
    "\n",
    "                if label is not None:\n",
    "                    matrix = self.znorm(matrix)\n",
    "                    matrix = matrix[:, ::downsample_factor]\n",
    "\n",
    "                    data.append(matrix)\n",
    "                    labels.append(label)  # Append the label directly\n",
    "                else:\n",
    "                    print(f\"Warning: No label found for file {file_name}\")\n",
    "\n",
    "        return np.array(data), np.array(labels)\n",
    "\n",
    "# Example usage\n",
    "data_loader = DataLoader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Intra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (32, 248, 8906)\n",
      "Labels Shape: (32,)\n",
      "Data Shape: (8, 248, 8906)\n",
      "Labels Shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "data_train, labels_train = data_loader.load_data_from_folder('./Final Project data/Intra/train')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Data Shape: {data_train.shape}\")\n",
    "print(f\"Labels Shape: {labels_train.shape}\")\n",
    "\n",
    "data_test, labels_test = data_loader.load_data_from_folder('./Final Project data/Intra/test')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Data Shape: {data_test.shape}\")\n",
    "print(f\"Labels Shape: {labels_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data: [nr samples, time steps, features]\n",
    "X_train = data_train\n",
    "X_test = data_test\n",
    "y_train = labels_train\n",
    "y_test = labels_test\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[2], X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[2], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m(y_train, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      2\u001b[0m y_test\u001b[38;5;241m=\u001b[39m to_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "y_train_encoded = to_categorical(y_train, num_classes=4)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Intra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build baseline LSTM model\n",
    "def build_baseline_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tuning the number of LSTM layers and their units\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 4)):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Choice('units_' + str(i), values=[16, 32, 64, 128]),\n",
    "            return_sequences=i < hp.get('num_lstm_layers') - 1,  # Only the last layer should not return sequences\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            dropout=hp.Choice('lstm_dropout_', values=[0.1, 0.2, 0.3, 0.4, 0.5])))\n",
    "\n",
    "    # Final Dense layer for classification\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Set up the tuner for hyperparameter optimization\n",
    "tuner = RandomSearch(\n",
    "    build_baseline_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='./',\n",
    "    project_name='baseline_lstm_hparam_tuning2'\n",
    ")\n",
    "\n",
    "# Start the hyperparameter tuning\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, batch_size=8, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train_encoded, epochs=50, batch_size=8, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping_callback])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To see the best hyper params + valuesL:\n",
    "print(\"Best Hyperparameters:\")\n",
    "for hyperparam in best_hps.values:\n",
    "    print(f\"{hyperparam}: {best_hps.get(hyperparam)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_encoded, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for additional metrics\n",
    "print(classification_report(np.argmax(y_test_encoded, axis=1), y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Intra model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning with LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tuning the number of LSTM layers and their units\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 4)):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Choice('units_' + str(i), values=[16, 32, 64, 128]), #values=[16, 32, 64, 128]\n",
    "            return_sequences=i < hp.get('num_lstm_layers') - 1,  # Only the last layer should not return sequences\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            dropout=hp.Choice('lstm_dropout_', values=[0.1, 0.2, 0.3, 0.4, 0.5])))\n",
    "\n",
    "    # Tuning the number of Dense layers and their units\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 4)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Choice('dense_units_' + str(i), values=[16, 32, 64, 128]), #values=[16, 32, 64, 128]\n",
    "            activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), #values=[1e-2, 1e-3, 1e-4]\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='./',\n",
    "    project_name='hparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, batch_size=8, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=8, validation_data=(X_test, y_test_encoded), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To see the best hyper params + valuesL:\n",
    "print(\"Best Hyperparameters:\")\n",
    "for hyperparam in best_hps.values:\n",
    "    print(f\"{hyperparam}: {best_hps.get(hyperparam)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions using model with best hyperparameters + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_encoded, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for additional metrics\n",
    "print(classification_report(np.argmax(y_test_encoded, axis=1), y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test accuracies for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run compile the model 10 times to get a distribution of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_best_hps(best_hps):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=best_hps['units_0'], return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=best_hps['lstm_dropout_']))\n",
    "    model.add(LSTM(units=best_hps['units_1'], return_sequences=best_hps['num_lstm_layers'] > 2))\n",
    "    if best_hps['num_lstm_layers'] > 2:\n",
    "        model.add(LSTM(units=best_hps['units_2']))\n",
    "    model.add(Dense(units=best_hps['dense_units_0'], activation='relu'))\n",
    "    model.add(Dense(units=best_hps['dense_units_1'], activation='relu'))\n",
    "    if best_hps['num_dense_layers'] > 2:\n",
    "        model.add(Dense(units=best_hps['dense_units_2'], activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_hps['learning_rate']), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "num_runs = 10\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Training Run {run+1}/{num_runs}\")\n",
    "\n",
    "    model = build_model_from_best_hps(best_hps)\n",
    "    model.fit(X_train, y_train_encoded, epochs=50, batch_size=8, validation_data=(X_test, y_test_encoded), callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], verbose=0)\n",
    "\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)\n",
    "\n",
    "    all_train_accuracies.append(train_accuracy)\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_model = tf.keras.models.clone_model(model)\n",
    "        best_model.set_weights(model.get_weights())\n",
    "\n",
    "print(f\"Average Training Accuracy: {np.mean(all_train_accuracies):.2f}%\")\n",
    "print(f\"Average Test Accuracy: {np.mean(all_test_accuracies):.2f}%\")\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# Confusion Matrix and Classification Report for the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(np.argmax(y_test_encoded, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(np.argmax(y_test_encoded, axis=1), y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and labels for each subset\n",
    "data_train1, labels_train1 = data_loader.load_data_from_folder('./Final Project data/Cross/train')\n",
    "data_test1, labels_test1 = data_loader.load_data_from_folder('./Final Project data/Cross/test1')\n",
    "data_test2, labels_test2 = data_loader.load_data_from_folder('./Final Project data/Cross/test2')\n",
    "data_test3, labels_test3 = data_loader.load_data_from_folder('./Final Project data/Cross/test3')\n",
    "\n",
    "# Print shapes of loaded data\n",
    "print(f\"Train Data Shape: {data_train1.shape}, Train Labels Shape: {labels_train1.shape}\")\n",
    "print(f\"Test1 Data Shape: {data_test1.shape}, Test1 Labels Shape: {labels_test1.shape}\")\n",
    "print(f\"Test2 Data Shape: {data_test2.shape}, Test2 Labels Shape: {labels_test2.shape}\")\n",
    "print(f\"Test3 Data Shape: {data_test3.shape}, Test3 Labels Shape: {labels_test3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshapen zodat het in de vorm: [nr samples, time steps, features] is, voor LSTM\n",
    "X_train = data_train1\n",
    "X_test1, X_test2, X_test3 = data_test1, data_test2, data_test3\n",
    "y_train = labels_train1\n",
    "y_test1, y_test2, y_test3 = labels_test1, labels_test2, labels_test3\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[2], X_train.shape[1]))\n",
    "X_test1, X_test2, X_test3 = X_test1.reshape((X_test1.shape[0], X_test1.shape[2], X_test1.shape[1])), X_test2.reshape((X_test2.shape[0], X_test2.shape[2], X_test2.shape[1])), X_test3.reshape((X_test3.shape[0], X_test3.shape[2], X_test3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = to_categorical(y_train, num_classes=4)\n",
    "y_test1_encoded, y_test2_encoded, y_test3_encoded = to_categorical(y_test1, num_classes=4), to_categorical(y_test2, num_classes=4), to_categorical(y_test3, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_res = np.concatenate((X_test2, X_test3), axis=0)\n",
    "y_test_res = np.concatenate((y_test2, y_test3), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cross Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tuning the number of LSTM layers and their units\n",
    "    for i in range(hp.Int('num_lstm_layers', 2, 4)):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Choice('units_' + str(i), values=[32, 64, 128, 256]), \n",
    "            return_sequences=i < hp.get('num_lstm_layers') - 1, \n",
    "            input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "            ))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5]))\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='./',\n",
    "    project_name='hparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Start the hyperparameter tuning\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, batch_size=8, validation_data=(X_test1, y_test1_encoded))\n",
    "# epochs + batch size voor nu constant (behalve epochs bij fitten van model)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(X_train, y_train_encoded, epochs=50, batch_size=8, validation_data=(X_test1, y_test1_encoded), callbacks=[early_stopping_callback])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_res, y_test_res)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To see the best hyper params + valuesL:\n",
    "print(\"Best Hyperparameters:\")\n",
    "for hyperparam in best_hps.values:\n",
    "    print(f\"{hyperparam}: {best_hps.get(hyperparam)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_res)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_res, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for additional metrics\n",
    "print(classification_report(np.argmax(y_test_res, axis=1), y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Cross Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning with LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tuning the number of LSTM layers and their units\n",
    "    for i in range(hp.Int('num_lstm_layers', 2, 4)):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Choice('units_' + str(i), values=[32, 64, 128, 256]), \n",
    "            return_sequences=i < hp.get('num_lstm_layers') - 1,  \n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            dropout=hp.Choice('lstm_dropout_', values=[0.2, 0.3, 0.4, 0.5]),\n",
    "            kernel_regularizer=l2(hp.Choice('lstm_kernel_regularizer_', values=[1e-3, 1e-4, 1e-5]))\n",
    "            ))\n",
    "\n",
    "    # Tuning the number of Dense layers and their units\n",
    "    for i in range(hp.Int('num_dense_layers', 2, 6)):\n",
    "        model.add(Dense(\n",
    "            units=hp.Choice('dense_units_' + str(i), values=[32, 64, 128, 256])\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(hp.Choice('dense_kernel_regularizer_', values=[1e-3, 1e-4, 1e-5]))))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5]))\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='./',\n",
    "    project_name='hparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Start the hyperparameter tuning\n",
    "tuner.search(X_train, y_train_encoded, epochs=10, batch_size=8, validation_data=(X_test1, y_test1_encoded))\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=32, validation_data=(X_test1, y_test1_encoded), callbacks=[early_stopping_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "for hyperparam in best_hps.values:\n",
    "    print(f\"{hyperparam}: {best_hps.get(hyperparam)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions using model with best hyperparameters + confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = model.predict(X_test_res)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(np.argmax(y_test_res, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report for additional metrics\n",
    "print(classification_report(np.argmax(y_test_res, axis=1), y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test accuracies for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on the training data\n",
    "train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded)\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run compile the model 10 times to get a distribution of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_from_best_hps(loaded_hyperparameters): \n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding LSTM layers as per loaded hyperparameters\n",
    "    for i in range(loaded_hyperparameters['num_lstm_layers']):\n",
    "        model.add(LSTM(\n",
    "            units=loaded_hyperparameters[f'units_{i}'],\n",
    "            return_sequences=i < loaded_hyperparameters['num_lstm_layers'] - 1,\n",
    "            input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "            dropout=loaded_hyperparameters.get('lstm_dropout_', 0),\n",
    "            kernel_regularizer=l2(loaded_hyperparameters.get('lstm_kernel_regularizer_'))\n",
    "            ))\n",
    "\n",
    "    # Adding Dense layers as per loaded hyperparameters\n",
    "    for i in range(loaded_hyperparameters['num_dense_layers']):\n",
    "        model.add(Dense(\n",
    "            units=loaded_hyperparameters[f'dense_units_{i}'],\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(loaded_hyperparameters.get('dense_kernel_regularizer_'))\n",
    "            ))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(loaded_hyperparameters['learning_rate']),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 10\n",
    "all_train_accuracies = []\n",
    "all_test_accuracies = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"Training Run {run+1}/{num_runs}\")\n",
    "\n",
    "    model = build_model_from_best_hps(best_hps)\n",
    "    model.fit(X_train, y_train_encoded, epochs=50, batch_size=8, validation_data=(X_test1, y_test1_encoded), callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)], verbose=2)\n",
    "\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_res, y_test_res, verbose=0)\n",
    "\n",
    "    all_train_accuracies.append(train_accuracy)\n",
    "    all_test_accuracies.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_model = tf.keras.models.clone_model(model)\n",
    "        best_model.set_weights(model.get_weights())\n",
    "\n",
    "print(f\"Average Training Accuracy: {np.mean(all_train_accuracies):.2f}%\")\n",
    "print(f\"Average Test Accuracy: {np.mean(all_test_accuracies):.2f}%\")\n",
    "print(f\"Best Model Test Accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report for the best model\n",
    "y_pred = best_model.predict(X_test_res)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(np.argmax(y_test_res, axis=1), y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "print(classification_report(np.argmax(y_test_res, axis=1), y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
